{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aed476b"
      },
      "source": [
        "# Sign Language Recognition\n",
        "\n",
        "## Indian Sign Language Dataset"
      ],
      "id": "7aed476b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBwFZvrBg-o0",
        "outputId": "dc7c627e-b722-4d99-b413-a8ac35d1709c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia: command not found\n"
          ]
        }
      ],
      "source": [
        "! nvidia -smi"
      ],
      "id": "gBwFZvrBg-o0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcF4guFvfBZp",
        "outputId": "7bdf0588-145a-4455-88ad-46d811f84bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "wcF4guFvfBZp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70adbe7f"
      },
      "source": [
        "How to improve the model?\n",
        "\n",
        "- Data normalization\n",
        "- Data augmentation\n",
        "- Residual connections\n",
        "- Batch normalization\n",
        "- Learning rate scheduling\n",
        "- Weight Decay\n",
        "- Gradient clipping\n",
        "- Adam optimizer"
      ],
      "id": "70adbe7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e82d484"
      },
      "source": [
        "### How to run the code\n",
        "\n",
        "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
        "\n",
        "#### Option 1: Running using free online resources (1-click, recommended)\n",
        "\n",
        "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Colab**. [Google Colab](https://colab.research.google.com) is a free online platform for running Jupyter notebooks using Google's cloud infrastructure. You can also select \"Run on Binder\" or \"Run on Kaggle\" if you face issues running the notebook on Google Colab. \n",
        "\n",
        "\n",
        "#### Option 2: Running on your computer locally\n",
        "\n",
        "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions."
      ],
      "id": "6e82d484"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d66fc59"
      },
      "source": [
        "### Using a GPU for faster training\n",
        "\n",
        "You can use a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) to train your models faster if your execution platform is connected to a GPU manufactured by NVIDIA. Follow these instructions to use a GPU on the platform of your choice:\n",
        "\n",
        "* _Google Colab_: Use the menu option \"Runtime > Change Runtime Type\" and select \"GPU\" from the \"Hardware Accelerator\" dropdown.\n",
        "* _Kaggle_: In the \"Settings\" section of the sidebar, select \"GPU\" from the \"Accelerator\" dropdown. Use the button on the top-right to open the sidebar.\n",
        "* _Binder_: Notebooks running on Binder cannot use a GPU, as the machines powering Binder aren't connected to any GPUs.\n",
        "* _Linux_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
        "* _Windows_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
        "* _macOS_: macOS is not compatible with NVIDIA GPUs\n",
        "\n",
        "\n",
        "If you do not have access to a GPU or aren't sure what it is, don't worry, you can execute all the code in this tutorial just fine without a GPU."
      ],
      "id": "7d66fc59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a8d7f16"
      },
      "source": [
        "Let's begin by installing and importing the required libraries."
      ],
      "id": "2a8d7f16"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "164570bf"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
        "\n",
        "# Linux / Binder / Windows (No GPU)\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Linux / Windows (GPU)\n",
        "# pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        " \n",
        "# MacOS (NO GPU)\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ],
      "id": "164570bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ae3c1a"
      },
      "outputs": [],
      "source": [
        "# Execute this cell if you're running this in colab\n",
        "# !pip install opendatasets --quiet"
      ],
      "id": "b4ae3c1a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f092027"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# import opendatasets as od\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ],
      "id": "5f092027"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c56a694"
      },
      "outputs": [],
      "source": [
        "project_name='indian-sign-language'"
      ],
      "id": "0c56a694"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e9079ca"
      },
      "source": [
        "## Preparing the Indian Sign Language Dataset\n",
        "\n",
        "Descritpion\n",
        "\n",
        "![image-here](https://i.imgur.com/xB0s72x.png)\n",
        "\n",
        "\n",
        "Let's begin by downloading the dataset and creating PyTorch datasets to load the data."
      ],
      "id": "8e9079ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "320f47be"
      },
      "outputs": [],
      "source": [
        "# Dowload the dataset\n",
        "# Execute this cell if you're running this in colab\n",
        "# dataset_url = \"https://www.kaggle.com/vaishnaviasonawane/indian-sign-language-dataset\"\n",
        "# od.download(dataset_url)"
      ],
      "id": "320f47be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "135cdcb9"
      },
      "outputs": [],
      "source": [
        "# Look into the data directory\n",
        "# Change the directory as required\n",
        "data_dir = '/content/drive/MyDrive/dataset_sign_language/'\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)"
      ],
      "id": "135cdcb9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c7d4034"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Let's evaluate a single class say \"A\"\n",
        "A_file=os.listdir(data_dir+\"A\")\n",
        "print(\"NO. of Training examples for Man:\",len(A_file))\n",
        "print(A_file[:5])"
      ],
      "id": "8c7d4034"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b357f961"
      },
      "outputs": [],
      "source": [
        "di={}\n",
        "for i in classes:\n",
        "    di[i]=len(os.listdir(data_dir+i))\n",
        "print(di)"
      ],
      "id": "b357f961"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88255c5a"
      },
      "source": [
        "First let's see a batch of raw images from the dataset. We will just import the dataloader to load the raw images. We will also create a helper function that will show us a batch of the image if we pass a DataLoader in it."
      ],
      "id": "88255c5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "957df79e"
      },
      "outputs": [],
      "source": [
        "target_num = len(classes)\n",
        "target_num"
      ],
      "id": "957df79e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1273ecc3"
      },
      "outputs": [],
      "source": [
        "# THe below function will print a batch of 64 images from the dataset\n",
        "\n",
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
        "        break"
      ],
      "id": "1273ecc3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727825dd"
      },
      "outputs": [],
      "source": [
        "raw_images = ImageFolder(data_dir, tt.ToTensor())"
      ],
      "id": "727825dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1b6bda"
      },
      "source": [
        "Let's see how a single image looks."
      ],
      "id": "0b1b6bda"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43830f60"
      },
      "outputs": [],
      "source": [
        "image, label = raw_images[0]\n",
        "print(\"Dimension:\", image.shape)\n",
        "plt.imshow(image.permute(1, 2, 0))"
      ],
      "id": "43830f60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdd9c740"
      },
      "outputs": [],
      "source": [
        "# Let's create a dataloader for raw images\n",
        "raw_dl = DataLoader(raw_images, 400, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "id": "fdd9c740"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef1f15ca"
      },
      "outputs": [],
      "source": [
        "show_batch(raw_dl)"
      ],
      "id": "ef1f15ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2152c2b"
      },
      "source": [
        "The above batch is a batch of image without any preprocessing or modification to the data.\n",
        "\n",
        "Now We will create training and validation datasets using the `ImageFolder` class from `torchvision`. In addition to the `ToTensor` transform, we'll also apply some other transforms to the images. There are a few important changes we'll make while creating PyTorch datasets for training and validation:\n",
        "\n",
        "\n",
        "1. **Channel-wise data normalization**: We will normalize the image tensors by subtracting the mean and dividing by the standard deviation across each channel. As a result, the mean of the data across each channel is 0, and standard deviation is 1. Normalizing the data prevents the values from any one channel from disproportionately affecting the losses and gradients while training, simply by having a higher or wider range of values that others.\n",
        "\n",
        "<img src=\"https://i.imgur.com/LYxXBVg.png\" width=\"360\">\n",
        "\n",
        "\n",
        "2. **Randomized data augmentations**: We will apply randomly chosen transformations while loading images from the training dataset. Specifically, we will pad each image by 4 pixels, and then take a random crop of size 32 x 32 pixels, and then flip the image horizontally with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better.\n",
        "\n",
        "![data-augmentation](https://imgaug.readthedocs.io/en/latest/_images/cropandpad_percent.jpg)"
      ],
      "id": "c2152c2b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b7ce45b"
      },
      "source": [
        "First let's get the values for channel-wise data normalization. We will subtract the mean and divide the standard deviation across each channel"
      ],
      "id": "7b7ce45b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e5bb01d"
      },
      "outputs": [],
      "source": [
        "average  = torch.Tensor([0,0,0])\n",
        "standard_dev = torch.Tensor([0,0,0])\n",
        "\n",
        "for image, labels in raw_images:\n",
        "    average += image.mean([1,2])\n",
        "    standard_dev += image.std([1,2])\n",
        "stats_avgs = (average / len(raw_images)).tolist()\n",
        "stats_stds =  (standard_dev / len(raw_images)).tolist()\n",
        "stats_avgs, stats_stds"
      ],
      "id": "9e5bb01d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54035de0"
      },
      "outputs": [],
      "source": [
        "# Data transforms (normalization & data augmentation)\n",
        "\n",
        "stats = (stats_avgs, stats_stds)\n",
        "train_tfms = tt.Compose([# tt.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                         tt.RandomHorizontalFlip(), \n",
        "                         # tt.RandomRotate\n",
        "                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n",
        "                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "                         tt.ToTensor(), \n",
        "                         tt.Normalize(*stats,inplace=True)])\n",
        "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
      ],
      "id": "54035de0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa1f5c91"
      },
      "source": [
        "Now we will split the datset into train_ds and valid_ds."
      ],
      "id": "fa1f5c91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7fcf21"
      },
      "outputs": [],
      "source": [
        "train_data = ImageFolder(data_dir, transform=train_tfms)\n",
        "valid_data = ImageFolder(data_dir, transform=valid_tfms)\n",
        "test_data = ImageFolder(data_dir, transform=valid_tfms)"
      ],
      "id": "5c7fcf21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac897e63"
      },
      "outputs": [],
      "source": [
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "# We'll fix the random state to avoid change in the creation of train and valid data.\n",
        "np.random.seed(42) \n",
        "np.random.shuffle(indices)\n",
        "# Let's take 15% of the train data as validation and 10% as test\n",
        "valid_size = 0.15\n",
        "test_size = 0.10\n",
        "val_split = int(np.floor(valid_size * num_train))\n",
        "test_split = int(np.floor(test_size * num_train))\n",
        "valid_idx, test_idx, train_idx = indices[:val_split], indices[val_split:val_split+test_split], indices[val_split+test_split:]"
      ],
      "id": "ac897e63"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6674cc"
      },
      "source": [
        "Next, we can create data loaders for retrieving images in batches. We'll use a  batch size of 124 for now. We might increase the batch size later to utlize a larger portion of the GPU RAM. We can also reduce the batch size if we face an \"out of memory\" error."
      ],
      "id": "df6674cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1066abd3"
      },
      "outputs": [],
      "source": [
        "batch_size = 250"
      ],
      "id": "1066abd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31d589f3"
      },
      "outputs": [],
      "source": [
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "# test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "# prepare data loaders \n",
        "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=train_sampler, num_workers=2, pin_memory=True)\n",
        "valid_dl = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
        "    sampler=valid_sampler, num_workers=2, pin_memory=True)\n",
        "# test_dl = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
        "#     sampler=test_sampler, num_workers=2, pin_memory=True)"
      ],
      "id": "31d589f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf9401c4"
      },
      "outputs": [],
      "source": [
        "# Let's remove the unwanted variables from memory\n",
        "del raw_images, average, standard_dev, raw_dl"
      ],
      "id": "bf9401c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbdd4b6"
      },
      "source": [
        "Let's take a look at some sample images from the training dataloader. To display the images, we'll need to _denormalize_ the pixels values to bring them back into the range `(0,1)`."
      ],
      "id": "dfbdd4b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e8435b5"
      },
      "outputs": [],
      "source": [
        "def denormalize(images, means, stds):\n",
        "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
        "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
        "    return images * stds + means\n",
        "\n",
        "def show_batch(dl, denorm=False):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        if denorm:\n",
        "            images = denormalize(images, *stats)\n",
        "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n",
        "        break"
      ],
      "id": "2e8435b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc2a0a20"
      },
      "outputs": [],
      "source": [
        "# Normalized and Augmented Image\n",
        "show_batch(train_dl)"
      ],
      "id": "bc2a0a20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "139fac69"
      },
      "outputs": [],
      "source": [
        "# Original Image\n",
        "show_batch(train_dl, denorm=True)"
      ],
      "id": "139fac69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00cb5dd2"
      },
      "outputs": [],
      "source": [
        "show_batch(valid_dl)"
      ],
      "id": "00cb5dd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a726401c"
      },
      "source": [
        "The colors seem out of place because of the normalization. Note that normalization is also applied during inference. If you look closely, you can see the cropping and reflection padding in some of the images. Horizontal flip is a bit difficult to detect from visual inspection.\n",
        "\n",
        "## Using a GPU\n",
        "\n",
        "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required. These are described in more detail in a [previous tutorial](https://jovian.ml/aakashns/04-feedforward-nn#C21)."
      ],
      "id": "a726401c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96b63ed8"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "id": "96b63ed8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320e25f8"
      },
      "source": [
        "Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)"
      ],
      "id": "320e25f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dabd70a"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "id": "9dabd70a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba35a715"
      },
      "source": [
        "We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available)."
      ],
      "id": "ba35a715"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1264be5f"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)"
      ],
      "id": "1264be5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f7eda70"
      },
      "source": [
        "## Model with Residual Blocks and Batch Normalization\n",
        "\n",
        "One of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n",
        "\n",
        "![](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n",
        "\n",
        "Here is a very simple Residual block:"
      ],
      "id": "6f7eda70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c81f6997"
      },
      "outputs": [],
      "source": [
        "# class SimpleResidualBlock(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.relu1(out)\n",
        "#         out = self.conv2(out)\n",
        "#         return self.relu2(out) + x # ReLU can be applied before or after adding the input"
      ],
      "id": "c81f6997"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73490249"
      },
      "outputs": [],
      "source": [
        "# simple_resnet = to_device(SimpleResidualBlock(), device)\n",
        "\n",
        "# for images, labels in train_dl:\n",
        "#     out = simple_resnet(images)\n",
        "#     print(out.shape)\n",
        "#     break\n",
        "    \n",
        "# del simple_resnet, images, labels\n",
        "# torch.cuda.empty_cache()"
      ],
      "id": "73490249"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ffd97e"
      },
      "source": [
        "This seeming small change produces a drastic improvement in the performance of the model. Also, after each convolutional layer, we'll add a batch normalization layer, which normalizes the outputs of the previous layer. \n",
        "\n",
        "\n",
        "We will use the ResNet9 architecture,"
      ],
      "id": "70ffd97e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b46ab3f3"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "id": "b46ab3f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "811bb298"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64, pool=True) # 64 x 64 x 64\n",
        "        self.conv2 = conv_block(64, 128, pool=True) # 128 x 32 x 32\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) # 128 x 32 x 32\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True) # 256 x 16 x 16\n",
        "        self.conv4 = conv_block(256, 512, pool=True) # 512 x 8 x 8\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) # 512 x 8 x 8\n",
        "        self.conv5 = conv_block(512, 512, pool=True) # 512 x 4 x 4\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.conv5(out)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "id": "811bb298"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6c37c5f"
      },
      "outputs": [],
      "source": [
        "model = to_device(ResNet9(3, target_num), device)\n",
        "model"
      ],
      "id": "a6c37c5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e93ec39"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Before we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n",
        "\n",
        "* **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the **\"One Cycle Learning Rate Policy\"**, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https://sgugger.github.io/the-1cycle-policy.html\n",
        "\n",
        "* **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab\n",
        "\n",
        "* **Gradient clipping**: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
        "\n",
        "\n",
        "Let's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch."
      ],
      "id": "9e93ec39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfe400f"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "id": "dbfe400f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "299b64c9"
      },
      "outputs": [],
      "source": [
        "history = [evaluate(model, valid_dl)]\n",
        "history"
      ],
      "id": "299b64c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76c34ded"
      },
      "source": [
        "We're now ready to train our model. Instead of SGD (stochastic gradient descent), we'll use the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. You can learn more about optimizers here: https://ruder.io/optimizing-gradient-descent/index.html"
      ],
      "id": "76c34ded"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "298459bf"
      },
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam"
      ],
      "id": "298459bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "759a6678"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)"
      ],
      "id": "759a6678"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bcd676e"
      },
      "outputs": [],
      "source": [
        "train_time='10:00'"
      ],
      "id": "8bcd676e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbe1b6c"
      },
      "source": [
        "Our model trained to about **100% accuracy in under 10 minutes**! \n",
        "\n",
        "Let's plot the valdation set accuracies to study how the model improves over time."
      ],
      "id": "1fbe1b6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8312d3c6"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');"
      ],
      "id": "8312d3c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bf6a995"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history)"
      ],
      "id": "1bf6a995"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af367028"
      },
      "source": [
        "We can also plot the training and validation losses to study the trend."
      ],
      "id": "af367028"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9dc42f3"
      },
      "outputs": [],
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ],
      "id": "c9dc42f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ae015c"
      },
      "outputs": [],
      "source": [
        "plot_losses(history)"
      ],
      "id": "81ae015c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7828127b"
      },
      "source": [
        "It's clear from the trend that our model isn't overfitting to the training data just yet. Try removing batch normalization, data augmentation and residual layers one by one to study their effect on overfitting.\n",
        "\n",
        "Finally, let's visualize how the learning rate changed over time, batch-by-batch over all the epochs."
      ],
      "id": "7828127b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7fe701d"
      },
      "outputs": [],
      "source": [
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.');"
      ],
      "id": "b7fe701d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5ef11a"
      },
      "outputs": [],
      "source": [
        "plot_lrs(history)"
      ],
      "id": "bc5ef11a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5dbac33"
      },
      "source": [
        "As expected, the learning rate starts at a low value, and gradually increases for 30% of the iterations to a maximum value of `0.01`, and then gradually decreases to a very small value."
      ],
      "id": "b5dbac33"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ea3094"
      },
      "source": [
        "## Testing with individual images\n",
        "\n",
        "While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset of 10000 images."
      ],
      "id": "d8ea3094"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "068a318b"
      },
      "outputs": [],
      "source": [
        "def predict_image(img, model):\n",
        "    # Convert to a batch of 1\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    # Get predictions from model\n",
        "    yb = model(xb)\n",
        "    # Pick index with highest probability\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    # Retrieve the class label\n",
        "    return train_data.classes[preds[0].item()]"
      ],
      "id": "068a318b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa351ba2"
      },
      "outputs": [],
      "source": [
        "correct = [] \n",
        "for i in test_idx:\n",
        "    img, lab = test_data[i]\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    yb = model(xb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    correct.append(preds[0].item() == lab)\n",
        "print(f\"Accuracy [Test Data]: {sum(correct) / len(test_idx) * 100} %\")"
      ],
      "id": "fa351ba2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cedd48c3"
      },
      "outputs": [],
      "source": [
        "n_rows, n_cols, i = 3, 5, 1\n",
        "fig = plt.figure(figsize=(16,10))\n",
        "for index in test_idx[:15]:\n",
        "    img, label = test_data[index]\n",
        "    ax = fig.add_subplot(n_rows, n_cols, i)\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(img.permute(1, 2, 0).clamp(0,1))\n",
        "    ax.set_title(f\"Label: {test_data.classes[label]} , Predicted: {predict_image(img, model)}\")\n",
        "    i+=1"
      ],
      "id": "cedd48c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFEp3CdR7UrZ"
      },
      "outputs": [],
      "source": [],
      "id": "JFEp3CdR7UrZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1abfecc"
      },
      "outputs": [],
      "source": [
        "!pip install gtts"
      ],
      "id": "b1abfecc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0158bc24"
      },
      "outputs": [],
      "source": [
        "img, label = valid_data[19872]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "print('Label:', valid_data.classes[label], ', Predicted:', predict_image(img, model))"
      ],
      "id": "0158bc24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaQqENvOFvxc"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "st=predict_image(img, model)\n",
        "tts = gTTS('Pridicted letter is ',predict_image(img, model))\n",
        "tts.save('1.wav')\n",
        "sound_file = '1.wav'\n",
        "Audio(sound_file, autoplay=True)"
      ],
      "id": "EaQqENvOFvxc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d781370d"
      },
      "source": [
        "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters."
      ],
      "id": "d781370d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630bbfb0"
      },
      "source": [
        "## Save and Commit\n",
        "\n",
        "Let's save the weights of the model, record the hyperparameters, and commit our experiment to Jovian. As you try different ideas, make sure to record every experiment so you can look back and analyze the results."
      ],
      "id": "630bbfb0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50595a17"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'ISN-2-custom-resnet.pth')"
      ],
      "id": "50595a17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "113a03f2"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "filename = 'ISN-1-custom-resnet.sav'\n",
        "joblib.dump(model, filename)"
      ],
      "id": "113a03f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5d7cb7"
      },
      "source": [
        "## Summary\n",
        "\n",
        "\n",
        "Here's a summary of the different techniques used to improve our model performance and reduce the training time:\n",
        "\n",
        "\n",
        "* **Data normalization**: We normalized the image tensors by subtracting the mean and dividing by the standard deviation of pixels across each channel. Normalizing the data prevents the pixel values from any one channel from disproportionately affecting the losses and gradients.\n",
        "\n",
        "* **Data augmentation**: We applied random transformations while loading images from the training dataset. Specifically, we did a random horizontal flip with a 50% probability. \n",
        "\n",
        "* **Residual connections**: One of the key things to our CNN model was the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers. We used a ResNet9 like architecture \n",
        "\n",
        "* **Batch normalization**: After each convolutional layer, we added a batch normalization layer, which normalizes the outputs of the previous layer. This is somewhat similar to data normalization, except it's applied to the outputs of a layer, and the mean and standard deviation are learned parameters.\n",
        "\n",
        "* **Learning rate scheduling**: Instead of using a fixed learning rate, we used a learning rate scheduler, which changes the learning rate after every batch of training. There are [many strategies](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for varying the learning rate during training, and we used the \"One Cycle Learning Rate Policy\". [Learn more](https://sgugger.github.io/the-1cycle-policy.html)\n",
        "\n",
        "* **Weight Decay**: We added weight decay to the optimizer, yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function. [Learn more](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)\n",
        "\n",
        "* **Gradient clipping**: We also added gradient clippint, which helps limit the values of gradients to a small range to prevent undesirable changes in model parameters due to large gradient values during training.  [Learn more.](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48#63e0)\n",
        "\n",
        "* **Adam optimizer**: Instead of SGD (stochastic gradient descent), we used the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. There are many other optimizers to choose from and experiment with. [Learn more.](https://ruder.io/optimizing-gradient-descent/index.html)"
      ],
      "id": "fc5d7cb7"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 866.996873,
      "end_time": "2022-01-10T22:24:28.069429",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-01-10T22:10:01.072556",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}